{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 13)\n",
      "=====================Ordinary Least Squares================\n",
      "Training error = 0.753382168477, best is 1.0\n",
      "Test error = 0.41361486952, best is 1.0\n",
      "Mean absolute error: 1.00430339177, best is zero\n",
      "Mean squared error: 1.28792560099, best is zero\n",
      "=======================Ridge Regression==================\n",
      "Used alpha: 6.0\n",
      "Training error = 0.751511255242, best is 1.0\n",
      "Test error = 0.468877797759, best is 1.0\n",
      "Mean absolute error: 0.956079326283, best is zero\n",
      "Mean squared error: 1.16654711377, best is zero\n",
      "================Support Vector Regression====================\n",
      "Training error = 0.624159397097, best is 1.0\n",
      "Test error = 0.844488299782, best is 1.0\n",
      "Mean absolute error: 0.432320313648, best is zero\n",
      "Mean squared error: 0.341563060782, best is zero\n"
     ]
    }
   ],
   "source": [
    "from skrvm import RVR\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "import logging\n",
    "\n",
    "#TODO: check negatively predicted values\n",
    "#TODO: use different linear models\n",
    "#TODO: for comparison between different ML models use mean square error, for comparison with analytical model use accuracy: mean(predicted/true value * 100)\n",
    "\n",
    "logging.basicConfig(filename='prediction_results.log',filemode='w',level=logging.WARNING)\n",
    "\n",
    "#add matmul gpu uncoalased ..\n",
    "#========================Preparing the data===================================#\n",
    "\n",
    "#Import all dataset\n",
    "dataset = np.genfromtxt('dataset_matMul.csv', dtype=float, delimiter=',', skip_header =1);\n",
    "print dataset.shape;\n",
    "\n",
    "#Calculate: number of samples,features, output index, training set size ..\n",
    "featureStart = 1;\n",
    "samplesCount = dataset.shape[0];\n",
    "columnsCount = dataset.shape[1];\n",
    "featuresCount = dataset.shape[1]-1;\n",
    "outputIndex = dataset.shape[1]-1;\n",
    "\n",
    "trainingSetCount = int(80 * samplesCount /100);\n",
    "#If I needed to overwrite it\n",
    "trainingSetCount = 530;\n",
    "\n",
    "#print trainingSetCount\n",
    "#For training set: separate the feature set from the target attributes\n",
    "X = dataset[featureStart:trainingSetCount,featureStart:featuresCount]; # last one not included\n",
    "y = dataset[featureStart:trainingSetCount,outputIndex];\n",
    "\n",
    "#True Output values that will be used in calcuating the accuracy of prediction\n",
    "y_true = dataset[trainingSetCount+1:samplesCount,outputIndex]\n",
    "#print y_true\n",
    "\n",
    "#Scale values with mean = zero and standard deviation =1\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_std = std_scale.transform(X)\n",
    "#print X_std;\n",
    "\n",
    "#Scale test set\n",
    "X_val = dataset[trainingSetCount+1:samplesCount,featureStart:featuresCount];\n",
    "X_val_std = std_scale.transform(X_val)\n",
    "#print X_val_std;\n",
    "\n",
    "#=====================Ordinary Least Squares Linear model======================================\n",
    "print \"=====================Ordinary Least Squares================\"\n",
    "#Training phase\n",
    "lr = linear_model.LinearRegression();\n",
    "lr.fit(X_std,y);\n",
    "\n",
    "training_error = lr.score(X_std,y);\n",
    "print \"Training error = \" + str(training_error) + \", best is 1.0\";\n",
    "\n",
    "test_error = lr.score(X_val_std,y_true );\n",
    "print \"Test error = \" + str(test_error) + \", best is 1.0\";\n",
    "\n",
    "#Prediction for test set\n",
    "y_pred = lr.predict(X_val_std)\n",
    "#print y_pred\n",
    "\n",
    "#Calculating prediction error\n",
    "error = mean_absolute_error(y_true,y_pred);\n",
    "print \"Mean absolute error: \" + str(error) + \", best is zero\";\n",
    "\n",
    "error = mean_squared_error(y_true, y_pred) \n",
    "print \"Mean squared error: \" + str(error) + \", best is zero\";\n",
    "\n",
    "#accuracy = np.mean( np.divide(y_pred,y_true) )\n",
    "#print accuracy\n",
    "#===================Ridge Regression==========================================\n",
    "print \"=======================Ridge Regression==================\"\n",
    "#Try different regularization parameters\n",
    "ridgeCV = linear_model.RidgeCV(alphas=[0.01,0.1,0.3,0.6, 1.0,3.0,6.0, 10.0])\n",
    "\n",
    "ridgeCV.fit(X_std,y);\n",
    "print \"Used alpha: \" + str(ridgeCV.alpha_);\n",
    "\n",
    "training_error = ridgeCV.score(X_std,y);\n",
    "print \"Training error = \" + str(training_error) + \", best is 1.0\";\n",
    "\n",
    "test_error = ridgeCV.score(X_val_std,y_true );\n",
    "print \"Test error = \" + str(test_error) + \", best is 1.0\";\n",
    "\n",
    "y_pred = ridgeCV.predict(X_val_std);\n",
    "\n",
    "#Calculating prediction error\n",
    "error = mean_absolute_error(y_true,y_pred);\n",
    "print \"Mean absolute error: \" + str(error) + \", best is zero\";\n",
    "\n",
    "error = mean_squared_error(y_true, y_pred) \n",
    "print \"Mean squared error: \" + str(error) + \", best is zero\";\n",
    "\n",
    "#accuracy = np.mean( np.divide(y_pred,y_true) )\n",
    "#print accuracy\n",
    "\n",
    "\n",
    "#=======================Support Vector Regression=============================\n",
    "print \"================Support Vector Regression====================\"\n",
    "svmR = svm.SVR(kernel='poly', degree=1, shrinking=True);\n",
    "\n",
    "svmR.fit(X_std,y);\n",
    "\n",
    "training_error = svmR.score(X_std,y);\n",
    "print \"Training error = \" + str(training_error) + \", best is 1.0\";\n",
    "\n",
    "test_error = svmR.score(X_val_std,y_true );\n",
    "print \"Test error = \" + str(test_error) + \", best is 1.0\";\n",
    "\n",
    "y_pred = svmR.predict(X_val_std);\n",
    "#print y_pred\n",
    "#Calculating prediction error\n",
    "error = mean_absolute_error(y_true,y_pred);\n",
    "print \"Mean absolute error: \" + str(error) + \", best is zero\";\n",
    "\n",
    "error = mean_squared_error(y_true, y_pred) \n",
    "print \"Mean squared error: \" + str(error) + \", best is zero\";\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
