alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
load("Dropbox/Doctorate/Results/MachineLearning/5.R.RData")
alpha.fn(Xy,1:100)
colnames(Xy) <- c("X", "XX", "Y")
alpha.fn(Xy,1:100)
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
new.rows = c(101:200, 401:500, 101:200, 901:1000, 301:400, 1:100, 1:100, 801:900, 201:300, 701:800)
new.Xy = Xy[new.rows, ]
alpha(Xy$X, Xy$Y)
alpha.fn(Portfolio,1:100)
alpha.fn (Xy,sample(1:100,100,replace=TRUE))
sample(1:100,100,replace=TRUE)
alpha.fn (Xy,1:100)
alpha.fn (Xy,1:1000)
alpha.fn (Xy,1:100)
alpha.fn (new.Xy ,1:100)
summary(lm(y~.,data=Xy))
summary(lm(Xy$Y~.,data=Xy))
############## 6.1 Subset Selection ########
data.credit=read.csv('http://www.mathstat.dal.ca/~aarms2014/StatLearn/data/Credit.csv',head=T)
data.credit=data.credit[,-1] # see Chapter 3 for the detailed explanation of this data set
names(data.credit)
# Best subset selection
library(leaps)
regfit.full=regsubsets(Balance~.,data.credit,nvmax=11)
reg.summary=summary(regfit.full)
par(mfrow=c(1,2))
plot(reg.summary$rss, xlab="Number of Predictors", ylab="Residual Sum of Squares",
type="l", xlim=c(0,12), ylim=c(min(reg.summary$rss), max(reg.summary$rss)))
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], cex=2, pch=2, col="red")
plot(reg.summary$rsq, xlab="Number of Predictors", ylab="R^2", type="l", xlim=c(0,12), ylim=c(0,1))
points(which.max(reg.summary$rsq),reg.summary$rsq[which.max(reg.summary$rsq)], cex=2, pch=2, col="red")
################ Figure 6.1 #################
regMat=expand.grid(c(TRUE,FALSE), c(TRUE,FALSE), c(TRUE,FALSE),
c(TRUE,FALSE),
c(TRUE,FALSE), c(TRUE,FALSE), c(TRUE,FALSE),
c(TRUE,FALSE),
c(TRUE,FALSE), c(TRUE,FALSE))
allModelsList=apply(regMat, 1, function(x) as.formula(
paste(c("Balance ~ 1", names(data.credit)[x]),
collapse=" + ")) )
allModelsResults=lapply(allModelsList,
function(x) lm(x, data=data.credit))
NoOfCoef=unlist(apply(regMat, 1, sum))
RSS = unlist(lapply(allModelsResults, function(x) sum((summary(x)$residuals)^2)))
R2 =unlist(lapply(allModelsResults, function(x) summary(x)$r.squared))
par(mfrow=c(1,2))
plot(0,0, ylim=c(min(RSS)-1, max(RSS)+1), xlim=c(1,11), xlab="Number of
predictors", ylab="Residual Sum of Squares", type="n")
points(NoOfCoef, RSS, pch=1)
plot(0,0, ylim=c(0, 1), xlim=c(1,11), xlab="Number of
predictors", ylab="R Square", type="n")
points(NoOfCoef, R2, pch=1)
################### End of Figure 6.1 ###############
# now we examine RSS, adjusted R^2, Cp and BIC together
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Predictors", ylab="Residual Sum of Squares",
type="l", xlim=c(0,12), ylim=c(min(reg.summary$rss), max(reg.summary$rss)))
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], cex=2, pch=20, col="red")
plot(reg.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l", xlim=c(0,12),
ylim=c(min(reg.summary$cp),max(reg.summary$cp)))
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], cex=2, pch=20, col="red")
plot(reg.summary$adjr2, xlab="Number of Predictors", ylab="Adjusted R Square", type="l", xlim=c(0,12), ylim=c(0,1))
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], cex=2, pch=20, col="red")
###### rescale bic using rss #####
new.bic = rep(0, 11)
for(i in 1:11){new.bic[i]=reg.summary$rss[i]*(400-i+log(400)*i)/((400-i)*400)}
bic.sqrt=sqrt(new.bic)
plot(new.bic, xlab="Number of Predictors", ylab="BIC", type="l", xlim=c(0,12),
ylim=c(min(new.bic),max(new.bic)))
points(which.min(new.bic), new.bic[which.min(new.bic)], cex=2, pch=20, col="red")
#######
# Forward stepwise selection
regfit.fwd=regsubsets(Balance~.,data=data.credit,nvmax=11,method="forward")
summary(regfit.fwd)
coef(regfit.fwd,7)
# Backward stepwise selection
regfit.bwd=regsubsets(Balance~.,data=data.credit,nvmax=11, method="backward")
summary(regfit.bwd,7)
# Using validation set approach and cross-validation
# Validation set approach
set.seed(1)
train=sample(c(TRUE, FALSE), nrow(data.credit), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Balance~., data=data.credit[train,],nvmax=11)
test.mat=model.matrix(Balance~.,data=data.credit[test,])
val.errors=rep(NA,11)
for(i in 1:11){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((data.credit$Balance[test]-pred)^2)
}
# Cross-validation
k=10
set.seed(1)
folds=sample(1:k,nrow(data.credit),replace=TRUE)
cv.errors=matrix(NA,k,11,dimnames=list(NULL,paste(1:11)))
predict.regsubsets=function(object,newdata,id){
form=as.formula(object$call[[2]])
mat=model.matrix(form, newdata)
coefi=coef(object, id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
for(j in 1:k){
best.fit=regsubsets(Balance~.,data=data.credit[folds!=j,], nvmax=11)
for(i in 1:11){
pred=predict(best.fit, data.credit[folds==j,], id=i)
cv.errors[j,i]=mean((data.credit$Balance[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors, 2, mean)
# Now we compare BIC, Validation Set Error, Cross-Validation Error in terms of their square roots
par(mfrow=c(1,3))
plot(bic.sqrt, xlab="Number of Predictors", ylab="Squared Root of BIC",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(new.bic), bic.sqrt[which.min(new.bic)], cex=2, pch=20, col="red")
plot(sqrt(val.errors), xlab="Number of Predictors", ylab="Validation Set Error",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(val.errors)), sqrt(val.errors)[which.min(sqrt(val.errors))], cex=2, pch=20, col="red")
plot(sqrt(mean.cv.errors), xlab="Number of Predictors", ylab="Cross-Validation Errors",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(mean.cv.errors)), sqrt(mean.cv.errors)[which.min(sqrt(mean.cv.errors))], cex=2, pch=20, col="red")
###### 6.2 Shrinkage Methods #######
#### Ridge regression ####
library(glmnet)
x=model.matrix(Balance~., data=data.credit)[,-1]
y=data.credit$Balance
grid=10^seq(-2, 4, length=100)
#For ridge regression, we need to standardize the predictors first
# the default is "standardize=TRUE"
ridge.mod=glmnet(x,y,alpha=0, lambda=grid)
plot(rev(ridge.mod$beta))
apply(ridge.mod$beta,1,summary)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[5]
coef(ridge.mod)[,5]
sqrt(sum(coef(ridge.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
ridge.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
ridge.pred=predict(ridge.refit,newx=x[test,])
mean((ridge.pred-y.test)^2)
#### LASSO ####
lasso.mod=glmnet(x,y,alpha=1, lambda=grid)
plot(rev(lasso.mod$beta))
apply(lasso.mod$beta,1,summary)
lasso.mod$lambda[50]
coef(lasso.mod)[,50]
sum(abs(coef(lasso.mod)[-1,50]^2))
lasso.mod$lambda[5]
coef(lasso.mod)[,5]
sum(abs(coef(lasso.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
lasso.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
lasso.pred=predict(lasso.refit,newx=x[test,])
mean((lasso.pred-y.test)^2)
############## 6.1 Subset Selection ########
data.credit=read.csv('http://www.mathstat.dal.ca/~aarms2014/StatLearn/data/Credit.csv',head=T)
data.credit=data.credit[,-1] # see Chapter 3 for the detailed explanation of this data set
names(data.credit)
# Best subset selection
library(leaps)
install.packages("leaps")
library(leaps)
regfit.full=regsubsets(Balance~.,data.credit,nvmax=11)
reg.summary=summary(regfit.full)
par(mfrow=c(1,2))
plot(reg.summary$rss, xlab="Number of Predictors", ylab="Residual Sum of Squares",
type="l", xlim=c(0,12), ylim=c(min(reg.summary$rss), max(reg.summary$rss)))
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], cex=2, pch=2, col="red")
plot(reg.summary$rsq, xlab="Number of Predictors", ylab="R^2", type="l", xlim=c(0,12), ylim=c(0,1))
points(which.max(reg.summary$rsq),reg.summary$rsq[which.max(reg.summary$rsq)], cex=2, pch=2, col="red")
################ Figure 6.1 #################
regMat=expand.grid(c(TRUE,FALSE), c(TRUE,FALSE), c(TRUE,FALSE),
c(TRUE,FALSE),
c(TRUE,FALSE), c(TRUE,FALSE), c(TRUE,FALSE),
c(TRUE,FALSE),
c(TRUE,FALSE), c(TRUE,FALSE))
allModelsList=apply(regMat, 1, function(x) as.formula(
paste(c("Balance ~ 1", names(data.credit)[x]),
collapse=" + ")) )
allModelsResults=lapply(allModelsList,
function(x) lm(x, data=data.credit))
NoOfCoef=unlist(apply(regMat, 1, sum))
RSS = unlist(lapply(allModelsResults, function(x) sum((summary(x)$residuals)^2)))
R2 =unlist(lapply(allModelsResults, function(x) summary(x)$r.squared))
par(mfrow=c(1,2))
plot(0,0, ylim=c(min(RSS)-1, max(RSS)+1), xlim=c(1,11), xlab="Number of
predictors", ylab="Residual Sum of Squares", type="n")
points(NoOfCoef, RSS, pch=1)
plot(0,0, ylim=c(0, 1), xlim=c(1,11), xlab="Number of
predictors", ylab="R Square", type="n")
points(NoOfCoef, R2, pch=1)
################### End of Figure 6.1 ###############
# now we examine RSS, adjusted R^2, Cp and BIC together
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Predictors", ylab="Residual Sum of Squares",
type="l", xlim=c(0,12), ylim=c(min(reg.summary$rss), max(reg.summary$rss)))
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], cex=2, pch=20, col="red")
plot(reg.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l", xlim=c(0,12),
ylim=c(min(reg.summary$cp),max(reg.summary$cp)))
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], cex=2, pch=20, col="red")
plot(reg.summary$adjr2, xlab="Number of Predictors", ylab="Adjusted R Square", type="l", xlim=c(0,12), ylim=c(0,1))
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], cex=2, pch=20, col="red")
###### rescale bic using rss #####
new.bic = rep(0, 11)
for(i in 1:11){new.bic[i]=reg.summary$rss[i]*(400-i+log(400)*i)/((400-i)*400)}
bic.sqrt=sqrt(new.bic)
plot(new.bic, xlab="Number of Predictors", ylab="BIC", type="l", xlim=c(0,12),
ylim=c(min(new.bic),max(new.bic)))
points(which.min(new.bic), new.bic[which.min(new.bic)], cex=2, pch=20, col="red")
#######
# Forward stepwise selection
regfit.fwd=regsubsets(Balance~.,data=data.credit,nvmax=11,method="forward")
summary(regfit.fwd)
coef(regfit.fwd,7)
# Backward stepwise selection
regfit.bwd=regsubsets(Balance~.,data=data.credit,nvmax=11, method="backward")
summary(regfit.bwd,7)
# Using validation set approach and cross-validation
# Validation set approach
set.seed(1)
train=sample(c(TRUE, FALSE), nrow(data.credit), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Balance~., data=data.credit[train,],nvmax=11)
test.mat=model.matrix(Balance~.,data=data.credit[test,])
val.errors=rep(NA,11)
for(i in 1:11){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((data.credit$Balance[test]-pred)^2)
}
# Cross-validation
k=10
set.seed(1)
folds=sample(1:k,nrow(data.credit),replace=TRUE)
cv.errors=matrix(NA,k,11,dimnames=list(NULL,paste(1:11)))
predict.regsubsets=function(object,newdata,id){
form=as.formula(object$call[[2]])
mat=model.matrix(form, newdata)
coefi=coef(object, id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
for(j in 1:k){
best.fit=regsubsets(Balance~.,data=data.credit[folds!=j,], nvmax=11)
for(i in 1:11){
pred=predict(best.fit, data.credit[folds==j,], id=i)
cv.errors[j,i]=mean((data.credit$Balance[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors, 2, mean)
# Now we compare BIC, Validation Set Error, Cross-Validation Error in terms of their square roots
par(mfrow=c(1,3))
plot(bic.sqrt, xlab="Number of Predictors", ylab="Squared Root of BIC",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(new.bic), bic.sqrt[which.min(new.bic)], cex=2, pch=20, col="red")
plot(sqrt(val.errors), xlab="Number of Predictors", ylab="Validation Set Error",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(val.errors)), sqrt(val.errors)[which.min(sqrt(val.errors))], cex=2, pch=20, col="red")
plot(sqrt(mean.cv.errors), xlab="Number of Predictors", ylab="Cross-Validation Errors",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(mean.cv.errors)), sqrt(mean.cv.errors)[which.min(sqrt(mean.cv.errors))], cex=2, pch=20, col="red")
###### 6.2 Shrinkage Methods #######
#### Ridge regression ####
library(glmnet)
x=model.matrix(Balance~., data=data.credit)[,-1]
y=data.credit$Balance
grid=10^seq(-2, 4, length=100)
#For ridge regression, we need to standardize the predictors first
# the default is "standardize=TRUE"
ridge.mod=glmnet(x,y,alpha=0, lambda=grid)
plot(rev(ridge.mod$beta))
apply(ridge.mod$beta,1,summary)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[5]
coef(ridge.mod)[,5]
sqrt(sum(coef(ridge.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
ridge.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
ridge.pred=predict(ridge.refit,newx=x[test,])
mean((ridge.pred-y.test)^2)
#### LASSO ####
lasso.mod=glmnet(x,y,alpha=1, lambda=grid)
plot(rev(lasso.mod$beta))
apply(lasso.mod$beta,1,summary)
lasso.mod$lambda[50]
coef(lasso.mod)[,50]
sum(abs(coef(lasso.mod)[-1,50]^2))
lasso.mod$lambda[5]
coef(lasso.mod)[,5]
sum(abs(coef(lasso.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
lasso.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
lasso.pred=predict(lasso.refit,newx=x[test,])
mean((lasso.pred-y.test)^2)
regfit.fwd=regsubsets(Balance~.,data=data.credit,nvmax=11,method="forward")
summary(regfit.fwd)
coef(regfit.fwd,7)
# Backward stepwise selection
regfit.bwd=regsubsets(Balance~.,data=data.credit,nvmax=11, method="backward")
summary(regfit.bwd,7)
# Using validation set approach and cross-validation
# Validation set approach
set.seed(1)
train=sample(c(TRUE, FALSE), nrow(data.credit), rep=TRUE)
test=(!train)
regfit.best=regsubsets(Balance~., data=data.credit[train,],nvmax=11)
test.mat=model.matrix(Balance~.,data=data.credit[test,])
val.errors=rep(NA,11)
for(i in 1:11){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((data.credit$Balance[test]-pred)^2)
}
# Cross-validation
k=10
set.seed(1)
folds=sample(1:k,nrow(data.credit),replace=TRUE)
cv.errors=matrix(NA,k,11,dimnames=list(NULL,paste(1:11)))
predict.regsubsets=function(object,newdata,id){
form=as.formula(object$call[[2]])
mat=model.matrix(form, newdata)
coefi=coef(object, id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
for(j in 1:k){
best.fit=regsubsets(Balance~.,data=data.credit[folds!=j,], nvmax=11)
for(i in 1:11){
pred=predict(best.fit, data.credit[folds==j,], id=i)
cv.errors[j,i]=mean((data.credit$Balance[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors, 2, mean)
# Now we compare BIC, Validation Set Error, Cross-Validation Error in terms of their square roots
par(mfrow=c(1,3))
plot(bic.sqrt, xlab="Number of Predictors", ylab="Squared Root of BIC",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(new.bic), bic.sqrt[which.min(new.bic)], cex=2, pch=20, col="red")
plot(sqrt(val.errors), xlab="Number of Predictors", ylab="Validation Set Error",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(val.errors)), sqrt(val.errors)[which.min(sqrt(val.errors))], cex=2, pch=20, col="red")
plot(sqrt(mean.cv.errors), xlab="Number of Predictors", ylab="Cross-Validation Errors",
type="b", xlim=c(0,12), ylim=c(100, 240))
points(which.min(sqrt(mean.cv.errors)), sqrt(mean.cv.errors)[which.min(sqrt(mean.cv.errors))], cex=2, pch=20, col="red")
###### 6.2 Shrinkage Methods #######
#### Ridge regression ####
library(glmnet)
x=model.matrix(Balance~., data=data.credit)[,-1]
y=data.credit$Balance
grid=10^seq(-2, 4, length=100)
#For ridge regression, we need to standardize the predictors first
# the default is "standardize=TRUE"
ridge.mod=glmnet(x,y,alpha=0, lambda=grid)
plot(rev(ridge.mod$beta))
apply(ridge.mod$beta,1,summary)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[5]
coef(ridge.mod)[,5]
sqrt(sum(coef(ridge.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
ridge.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
ridge.pred=predict(ridge.refit,newx=x[test,])
mean((ridge.pred-y.test)^2)
install.packages("glmnet")
library(glmnet)
x=model.matrix(Balance~., data=data.credit)[,-1]
y=data.credit$Balance
grid=10^seq(-2, 4, length=100)
#For ridge regression, we need to standardize the predictors first
# the default is "standardize=TRUE"
ridge.mod=glmnet(x,y,alpha=0, lambda=grid)
plot(rev(ridge.mod$beta))
apply(ridge.mod$beta,1,summary)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[5]
coef(ridge.mod)[,5]
sqrt(sum(coef(ridge.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
ridge.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
ridge.pred=predict(ridge.refit,newx=x[test,])
mean((ridge.pred-y.test)^2)
#### LASSO ####
lasso.mod=glmnet(x,y,alpha=1, lambda=grid)
plot(rev(lasso.mod$beta))
apply(lasso.mod$beta,1,summary)
lasso.mod$lambda[50]
coef(lasso.mod)[,50]
sum(abs(coef(lasso.mod)[-1,50]^2))
lasso.mod$lambda[5]
coef(lasso.mod)[,5]
sum(abs(coef(lasso.mod)[-1,5]^2))
### Estimate Test Errors using cross-validataion
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
lasso.refit=glmnet(x[train,],y[train],alpha=0,lambda=cv.out$lambda.1se)
lasso.pred=predict(lasso.refit,newx=x[test,])
mean((lasso.pred-y.test)^2)
getwd()
dirpath <- "~/Dropbox/Doctorate/Results/2016/svm-gpuperf/experiments/"
setwd(paste(dirpath, sep=""))
tracesName  <- read.csv(paste("./metricsName.csv",sep=","),header = TRUE)
dirpath <- "~/Dropbox/Doctorate/Results/2016/svm-gpuperf/experiments/"
setwd(paste(dirpath, sep=""))
eventsName  <- read.csv(paste("./eventsName.csv",sep=","),header = TRUE)
metricsName  <- read.csv(paste("./metricsName.csv",sep=","),header = TRUE)
tracesName  <- read.csv(paste("./metricsName.csv",sep=","),header = TRUE)
SSM_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/Subseqmax-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
Bit_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/bitonic_sort-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
DotP_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/dotProd-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMGPU_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMS_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMSU_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem_uncoalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMGPUUN_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_uncoalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
QS_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/quicksort-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
VAdd_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/vectorAdd-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MACo_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_coalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MAUn_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_normal-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
Metrics <- rbind(SSM_Metrics, DotP_Metrics,
MMGPU_Metrics, MMS_Metrics, MMSU_Metrics, MMGPUUN_Metrics,
QS_Metrics, VAdd_Metrics, MACo_Metrics, MAUn_Metrics )
dirpath <- "~/Dropbox/Doctorate/Results/2016/svm-gpuperf/experiments/"
setwd(paste(dirpath, sep=""))
eventsName  <- read.csv(paste("./eventsName.csv",sep=","),header = TRUE)
metricsName  <- read.csv(paste("./metricsName.csv",sep=","),header = TRUE)
tracesName  <- read.csv(paste("./metricsName.csv",sep=","),header = TRUE)
SSM_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/Subseqmax-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
Bit_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/bitonic_sort-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
DotP_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/dotProd-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMGPU_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMS_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMSU_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem_uncoalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MMGPUUN_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_uncoalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
QS_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/quicksort-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
VAdd_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/vectorAdd-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MACo_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_coalesced-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
MAUn_Metrics  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_normal-metrics.csv",sep=","),header = FALSE,col.names = names(metricsName)))
Metrics <- rbind(SSM_Metrics, DotP_Metrics,
MMGPU_Metrics, MMS_Metrics, MMSU_Metrics, MMGPUUN_Metrics,
QS_Metrics, VAdd_Metrics, MACo_Metrics, MAUn_Metrics )
View(Metrics)
View(Metrics)
SSM_Events  <- as.matrix(read.csv(paste("./Tesla-K40/Subseqmax-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
#Bit_Events  <- as.matrix(read.csv(paste("./Tesla-K40/bitonic_sort-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
DotP_Events  <- as.matrix(read.csv(paste("./Tesla-K40/dotProd-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MMGPU_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MMS_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MMSU_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_sharedmem_uncoalesced-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MMGPUUN_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matMul_gpu_uncoalesced-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
QS_Events  <- as.matrix(read.csv(paste("./Tesla-K40/quicksort-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
VAdd_Events  <- as.matrix(read.csv(paste("./Tesla-K40/vectorAdd-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MACo_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_coalesced-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
MAUn_Events  <- as.matrix(read.csv(paste("./Tesla-K40/matrix_sum_normal-events.csv",sep=","),header = FALSE, col.names = names(eventsName)))
Events <- rbind(SSM_Events,  DotP_Events,
MMGPU_Events, MMS_Events, MMSU_Events, MMGPUUN_Events,
QS_Events, VAdd_Events, MACo_Events, MAUn_Events )
