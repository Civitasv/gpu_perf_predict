\section{Related Works}\label{sec:relatedWorksML}
Some authors have focused their works on performance prediction of applications using machine learning~\citep{Jiangtian:2009, Ozisikyilmaz:2008, Singh:2007:PPA, Ipek:2005:APP,Matsunaga:2010}. The main learning algorithms that they used were statistical methods, Neural Networks, Support Vector Machines, and Random Forest.

In  recent  years,  studies on GPU performance using different statistical and machine learning approaches have appeared. ~\cite{Baldini:2014} showed that machine learning can predict GPU speedup from OpenMP applications. They used K-nearest neighbor and SVM as classifier to know the performance of these applications over different GPUs. 

In  recent  years, studies on comparison of GPU performance using analytical modeling, statistical analysis and machine learning approaches have appeared. ~\cite{Madougou:2016} presented a comparison between different GPGPU performance modeling tools, they compare between analytical model, statistical approaches, quantitative methods and compiler-based methods. In this works, authors concluded that the available GPU performance modeling solutions are very sensitive to applications and platform changes, and require
significant efforts for tuning and calibration when new analyses are required.

~\cite{Baldini:2014} showed that machine learning can predict GPU speedup of OpenCL applications from OpenMP applications. They used K-nearest neighbor and SVM as classifier to know the performance of these applications over different GPUs. They showed that a small set of easily-obtainable features can predict the magnitude of GPU speedups on two different high-end GPUs, with accuracies varying between 77\% and 90\%, depending on the prediction mechanism and scenario.

~\cite{Kerr:2012:Eiger} developed a methodology for the systematic construction of performance models of heterogeneous processors. They developed a framework, named Eiger, that implements their methodology. This methodology is comprised of experimental data acquisition and database construction, a series of data analysis passes over the database, and model selection and construction. They collected data from a simulator of GPU application named Ocelot, they defined a functional simulator~\cite{kerr2009characterization} for specific Nvidia PTX code. 

~\cite{Singh:2007:PPA} were one of the firsts authors to compare performance predictions of parallel computing models~\cite{Juurlink:1998}, comparing BSP, E-BSP and BPRAM over different parallel platform. Some authors have also focused their work in performance prediction of parallel applications using machine learning. All this work is about parallel applications executed over CPUs and not GPU applications.

~\cite{Greathouse:2015:GPGPUML} described a GPU performance and power estimation model, using K-means to create sets of scaling behaviors representative of the training kernels and neural networks that map kernels to clusters, with experiments using OpenCL applications over AMD GPUs. ~\cite{Karami:2013} proposed a statistical performance prediction model for OpenCL kernels on NVIDIA GPUs using a regression model for prediction and principle component analysis for extracting features of higher weights, thus reducing model complexity while preserving accuracy. 

~\cite{Hayashi:2015:MPH} presented  a statistical approach on  the  performance and power consumption of an ATI GPU~\cite{Zhang:2011}, using Random Forest due to its useful interpretation tools. Hayashi et al. constructed a prediction model that estimates the execution time of parallel applications based on a binary prediction model with Support Vector Machines for runtime CPU/GPU selection. 
%Kerr et al. developed Eiger~\cite{Kerr:2012:Eiger}, which is a framework for automated  statistical approaches for modeling program behaviors on diverse GPU architectures. They used various approaches, among them principal component analysis, clustering techniques, and regression analysis. %Madougou et al. presented a comparison between different GPGPU performance modeling tools~\cite{Madougou2016}, they compare between analytical model, statistical approaches, quantitative methods and compiler-based methods. 

~\cite{meswani:IPDPSW:2012} predicted the performance of HPC applications on hardware accelerators such as FPGA and GPU from applications running on CPU. This was done by identifying common compute patterns or idioms, then developing a framework to model the predicted speedup when the application is run on GPU or FPGA using these idioms. ~\cite{Ipek:2005:APP} trained multilayer neural networks to predict different performance aspects of parallel applications using input data from executing applications multiple times on the target platform.

In this work, we compare three different machine learning techniques to predict kernel execution times over NVIDIA GPUs. Moreover, we also perform a comparison with a BSP-based analytical model to verify when each approach is advantageous.




% In this work, we compare three different machine learning techniques to predict kernel execution times over NVIDIA GPUs. We also perform a comparison with a BSP-based analytical model to verify when each approach is advantageous. Although some works have compared analytical models, statistical approaches and quantitative methods, to the best of our knowledge this is the first work that compares analytical model to machine learning techniques to predict running times of GPU applications. Moreover, it offers a comparison between different machine learning techniques.